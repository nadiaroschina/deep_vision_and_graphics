{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bbdd602",
   "metadata": {},
   "source": [
    "# Seminar 11 - Stereo Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a8c17-58f8-4d8d-9d9a-330faa0fe60c",
   "metadata": {},
   "source": [
    "There are different variants of depth estimation task, depending on available input data. It can be\n",
    "- Rectified stereo image pair + ground truth\n",
    "- Non-rectified stereo image pair + ground truth\n",
    "- Stereo image pair **without** ground truth\n",
    "- ...\n",
    "\n",
    "The seminar will be about supervised setting, and in the homework you will go unsupervised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fb80f5",
   "metadata": {},
   "source": [
    "This task is based on two papers:\n",
    "\n",
    "1) Mayer et al. \"A Large Dataset to Train Convolutional Networksfor Disparity, Optical Flow, and Scene Flow Estimation\", CVPR 2016, ([pdf](https://openaccess.thecvf.com/content_cvpr_2016/papers/Mayer_A_Large_Dataset_CVPR_2016_paper.pdf), [poster](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/poster-MIFDB16.pdf), [supplimentary materials](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/supplementary-MIFDB16.pdf), [project page](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/)) \n",
    "\n",
    "2) Fischer at al. \"FlowNet: Learning Optical Flow with Convolutional Networks\", ICCV 2015, [pdf](https://arxiv.org/pdf/1504.06852.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c84910b-bde0-49a2-a6f8-e72d3db6d00b",
   "metadata": {},
   "source": [
    "First paper shows, that you can train depth estimation network on purely synthetical images and get strong performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438279f",
   "metadata": {},
   "source": [
    "<img src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Depth-estimation-DispNet.png?resize=768%2C599&ssl=1\" style=\"width:60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6497132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bfbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "def get_computing_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = get_computing_device()\n",
    "print(f\"Our main computing device is '{device}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080f80f",
   "metadata": {},
   "source": [
    "## 0. Warm up\n",
    "\n",
    "1) What is rectified stereo pair? What rectification is needed for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cf5a31",
   "metadata": {},
   "source": [
    "2) What is disparity? \n",
    "\n",
    "<details><summary>hint</summary>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*8RmW8h5XxSADXpJT_rXJAA.png\" style=\"width:60%\">\n",
    "<a href=\"https://www.gushiciku.cn/pl/2ThP\">img src</src>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26537a2d",
   "metadata": {},
   "source": [
    "3) How does disparity help with depth computation?\n",
    "\n",
    "<details><summary>hint</summary>\n",
    "<img src=\"https://i.stack.imgur.com/7RtcV.png\" style=\"width:60%\">\n",
    "<a href=\"https://stackoverflow.com/questions/56427239/convert-kinect-depth-intensity-to-distance-in-meter\">img src</a>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0403829-1b33-4d7a-931b-c24dd8bf9d60",
   "metadata": {},
   "source": [
    "Disparity is discrete and finite. How can we increase the maximal detectable depth?\n",
    "\n",
    "- increase baseline (you will probably lose objects which are too close, since they will be present only on one camera)\n",
    "- increase focus distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6e4b6-1178-48fd-b018-aee8822d9693",
   "metadata": {},
   "source": [
    "Note: we can also get X and Y coordinates with similar formulas, and reconstruct entire point cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b31bff7",
   "metadata": {},
   "source": [
    "## 1. KITTI Stereo Depth 2012\n",
    "\n",
    "http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo\n",
    "\n",
    "194 training image pairs, 195 test image pairs with hidden ground truth, ground truth depth captured by lidar.\n",
    "\n",
    "Zero disparity value means that disparity is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d6c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfile\n",
    "import os\n",
    "if not os.path.exists('./kitti_stereo_2012_training_data.zip'):\n",
    "    yfile.download_from_yadisk(\"https://disk.yandex.ru/d/WrA28IyHHYENOw\", \n",
    "                               \"kitti_stereo_2012_training_data.zip\",\n",
    "                               target_dir='.')\n",
    "\n",
    "# alternative, but looks like this way of downloading from g.drive is not working anymore for large files\n",
    "#import gfile\n",
    "#gfile.download_list(\n",
    "#    'https://drive.google.com/file/d/12zitJCsOVmoCHII5Ym_t2AAORXb6WMyU',\n",
    "#    filename='kitti_stereo_2012_training_data.zip',\n",
    "#    target_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9d0d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!unzip -q ./kitti_stereo_2012_training_data.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d7df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_disparity(img):\n",
    "    img = img.astype(np.float32) / 256\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47185c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = \"000002_10.png\"\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('left image')\n",
    "plt.imshow(Image.open(f'./kitti_stereo_2012_training_data/train/colored_0/{img_name}')); \n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('right image')\n",
    "plt.imshow(Image.open(f'./kitti_stereo_2012_training_data/train/colored_1/{img_name}')); \n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('disparity')\n",
    "disp = np.array(Image.open(f'./kitti_stereo_2012_training_data/train/disp_noc/{img_name}'))\n",
    "plt.imshow(normalize_disparity(disp), 'gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "           \n",
    "plt.subplot(2,2,4)\n",
    "plt.title('valid disparity mask')\n",
    "plt.imshow(disp > 0, 'gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac19c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_max_disparity = normalize_disparity(disp).max()\n",
    "sample_shape = disp.shape\n",
    "\n",
    "print(f'max disp = {sample_max_disparity} , disp shape {sample_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904040c",
   "metadata": {},
   "source": [
    "## 2. Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0202cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kitti_dataset import KITTIStereoRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b27b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get info about dataset implementation\n",
    "# KITTIStereoRAM??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a5867",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.array([0.35715697, 0.37349922, 0.35886646] , dtype=np.float32)\n",
    "stds = np.array([0.27408948, 0.2807328,  0.27994434], dtype=np.float32)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "# Q: can we use RandomFlip?\n",
    "# Q: can we use RandomRotation?\n",
    "# Q: can we use RandomCrop?\n",
    "\n",
    "# min kitti shape is [370, 1226], max shape is [376, 1242]\n",
    "PAD_HEIGHT = 128*3 \n",
    "PAD_WIDTH = 1280\n",
    "CROP_WIDTH = 768\n",
    "def transforms_train(left_image, right_image, disparity, valid_pixels_mask):\n",
    "    disparity = torchvision.transforms.functional.to_tensor(disparity)\n",
    "    valid_pixels_mask = torchvision.transforms.functional.to_tensor(valid_pixels_mask)\n",
    "    left_image = transform_train(left_image)\n",
    "    right_image = transform_train(right_image)\n",
    "    left_image = pad_to_size(left_image, PAD_HEIGHT, PAD_WIDTH)\n",
    "    right_image = pad_to_size(right_image, PAD_HEIGHT, PAD_WIDTH)\n",
    "    disparity = pad_to_size(disparity, PAD_HEIGHT, PAD_WIDTH)\n",
    "    valid_pixels_mask = pad_to_size(valid_pixels_mask, PAD_HEIGHT, PAD_WIDTH)\n",
    "\n",
    "    shift = torch.randint(0, PAD_WIDTH-CROP_WIDTH, (1,))\n",
    "    left_image = left_image[:,:,shift:shift+CROP_WIDTH]\n",
    "    right_image = right_image[:, :, shift: shift+CROP_WIDTH]\n",
    "    disparity = disparity[:, :, shift: shift+ CROP_WIDTH]\n",
    "    valid_pixels_mask = valid_pixels_mask[:, :, shift: shift+CROP_WIDTH]\n",
    "    return left_image, right_image, disparity, valid_pixels_mask\n",
    "\n",
    "\n",
    "def pad_to_size(images, min_height, min_width):\n",
    "    if images.shape[1] < min_height:\n",
    "        images = torchvision.transforms.functional.pad(images, (0,0,0,min_height-images.shape[1]))\n",
    "    if images.shape[2] < min_width:\n",
    "        images = torchvision.transforms.functional.pad(images, (0,0, min_width - images.shape[2], 0))\n",
    "    return images\n",
    "        \n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "def transforms_test(left_image, right_image, disparity, valid_pixels_mask):\n",
    "    disparity = torchvision.transforms.functional.to_tensor(disparity)\n",
    "    valid_pixels_mask = torchvision.transforms.functional.to_tensor(valid_pixels_mask)\n",
    "    left_image = transform_test(left_image)\n",
    "    right_image = transform_test(right_image)\n",
    "    left_image = pad_to_size(left_image, PAD_HEIGHT, PAD_WIDTH)\n",
    "    right_image = pad_to_size(right_image, PAD_HEIGHT, PAD_WIDTH)\n",
    "    disparity = pad_to_size(disparity, PAD_HEIGHT, PAD_WIDTH)\n",
    "    valid_pixels_mask = pad_to_size(valid_pixels_mask, PAD_HEIGHT, PAD_WIDTH)\n",
    "    \n",
    "    return left_image, right_image, disparity, valid_pixels_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = KITTIStereoRAM(root=\"./kitti_stereo_2012_training_data/\", train=True, transforms=transforms_train)\n",
    "\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_loader, \n",
    "                                              batch_size=4,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=16)\n",
    "val_loader = KITTIStereoRAM(root=\"./kitti_stereo_2012_training_data/\", train=False, transforms=transforms_test)\n",
    "\n",
    "val_batch_gen = torch.utils.data.DataLoader(val_loader, \n",
    "                                              batch_size=1,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d23ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in train_batch_gen:\n",
    "    left, right, target, mask = elem\n",
    "\n",
    "    print(\"left.shape\", left.shape)\n",
    "    print(\"right.shape\", right.shape)\n",
    "    print(\"target.shape\", target.shape)\n",
    "    print(\"mask.shape\", mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb13774",
   "metadata": {},
   "source": [
    "## 3. DispNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d95702",
   "metadata": {},
   "source": [
    "[Paper](https://openaccess.thecvf.com/content_cvpr_2016/papers/Mayer_A_Large_Dataset_CVPR_2016_paper.pdf), [poster](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/poster-MIFDB16.pdf), [supplimentary materials](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/supplementary-MIFDB16.pdf), [project page](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f19486-97ff-4043-9d87-5ef7e0b162da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: how would you solve this task, knowing the dataset? E.g. which loss, how to consider two images, which architecture to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ab994",
   "metadata": {},
   "source": [
    "### 3.1 DispNet Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb787bc",
   "metadata": {},
   "source": [
    "The simplest way to predict the disparity is just concat pair of images and feed it to unet-like architecture.\n",
    "\n",
    "[[FlowNet paper]](https://arxiv.org/pdf/1504.06852.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e0b426",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2400/0*LPtmtLr-mugr8OtN.png\" style=\"width:80%\">\n",
    "<img src=\"https://miro.medium.com/max/692/0*blFDiciN3KbPNeov.png\" style=\"width:80%\">\n",
    "\n",
    "Network architecture in more details:\n",
    "\n",
    "<img src=\"./dispnet.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfb3a2a-9369-48d7-979a-8bb37940f8ff",
   "metadata": {},
   "source": [
    "Note: why we predict from several scales?\n",
    "\n",
    "Answer: to help gradient propagation. Otherwise network might only use skip-connections from the very beginning. Also, it adds smoothing inductive bias -- for adjacent pixels depth is often similar (however, it's not true for edges). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNRelu(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_channels, out_channels, *args, **kwargs)\n",
    "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class UpConvBNRelu(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.ConvTranspose2d(in_channels, out_channels, *args, **kwargs)\n",
    "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class DispNetSimple(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBNRelu(6, 64, kernel_size=(7,7), stride=2, padding=(3,3))\n",
    "        self.conv2 = ConvBNRelu(64, 128, kernel_size=(5,5), stride=2, padding=(2,2))\n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            ConvBNRelu(128, 256, kernel_size=(5,5), stride=2, padding=(2,2)),\n",
    "            ConvBNRelu(256, 256, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.conv4 = torch.nn.Sequential(\n",
    "            ConvBNRelu(256, 512, kernel_size=(3,3), stride=2, padding=(1,1)),\n",
    "            ConvBNRelu(512, 512, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.conv5 = torch.nn.Sequential(\n",
    "            ConvBNRelu(512, 512, kernel_size=(3,3), stride=2, padding=(1,1)),\n",
    "            ConvBNRelu(512, 512, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.conv6 = torch.nn.Sequential(\n",
    "            ConvBNRelu(512, 1024, kernel_size=(3,3), stride=2, padding=(1,1)),\n",
    "            ConvBNRelu(1024, 1024, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.pred6 = torch.nn.Conv2d(1024, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "        \n",
    "        self.upconv5 = UpConvBNRelu(1024, 512, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv5 = ConvBNRelu(1025, 512, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred5 = torch.nn.Conv2d(512, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "\n",
    "        self.upconv4 = UpConvBNRelu(512, 256, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv4 = ConvBNRelu(256+512+1, 256, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred4 = torch.nn.Conv2d(256, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "        \n",
    "        self.upconv3 = UpConvBNRelu(256, 128, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv3 = ConvBNRelu(128+256+1, 128, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred3 = torch.nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "\n",
    "        self.upconv2 = UpConvBNRelu(128, 64, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv2 = ConvBNRelu(64+128+1, 64, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred2 = torch.nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "\n",
    "        self.upconv1 = UpConvBNRelu(64, 32, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv1 = ConvBNRelu(32+64+1, 32, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred1 = torch.nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "\n",
    "        # Here modules are first defined as class fields and then combined into a tuple.\n",
    "        # This way torch register them properly (for example, they occur in model.parameters() exactly once).\n",
    "        # You could avoid defining modules as class fields using nn.ModuleList.\n",
    "        self.encoder_layers = (self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.conv6)\n",
    "        self.decoder_layers = (\n",
    "            (self.upconv5, self.iconv5, self.pred5),\n",
    "            (self.upconv4, self.iconv4, self.pred4),\n",
    "            (self.upconv3, self.iconv3, self.pred3),\n",
    "            (self.upconv2, self.iconv2, self.pred2),\n",
    "            (self.upconv1, self.iconv1, self.pred1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, left_img, right_img):\n",
    "        x = torch.cat([left_img, right_img], dim=1)\n",
    "\n",
    "        # TODO apply dispnet\n",
    "\n",
    "        return predictions_per_scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419369ba",
   "metadata": {},
   "source": [
    "Let's check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f7977",
   "metadata": {},
   "outputs": [],
   "source": [
    "dispnet = DispNetSimple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ae74b-c8fa-48df-8a9a-3c1a4f240e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{sum(p.numel() for p in dispnet.parameters()) / 1000_000:.1f} million parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b06f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_batch_gen:\n",
    "    left, right, target, mask = sample\n",
    "    res = dispnet(left, right)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f2044-6d48-42e0-9284-0456c659bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "[pred.shape for pred in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890cc6b5",
   "metadata": {},
   "source": [
    "### 3.2 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364695a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predicted, target, mask):\n",
    "    losses = []\n",
    "    target_masked = target[mask]\n",
    "    for scale_pred in predicted:\n",
    "        scale_pred = torch.nn.functional.interpolate(\n",
    "            scale_pred, size=target.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        scale_pred = scale_pred[mask]\n",
    "        losses.append(torch.nn.functional.huber_loss(scale_pred, target_masked))\n",
    "    total_loss = sum(losses) / len(losses)\n",
    "    return total_loss, losses\n",
    "\n",
    "# Q: can we downsample ground truth instead of upsampling predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449dadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_loss(res, target, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13164c00",
   "metadata": {},
   "source": [
    "### 3.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "dispnet = DispNetSimple()\n",
    "dispnet = dispnet.to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(dispnet.parameters(), lr=4e-3, weight_decay=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c6fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, opt, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        train_scale_losses = []\n",
    "        val_scale_losses = []\n",
    "\n",
    "        network.train(True)\n",
    "\n",
    "        for x_left, x_right, gt, valid_pixels_mask in tqdm.tqdm(train_batch_gen):\n",
    "            opt.zero_grad()\n",
    "            x_left = x_left.to(device)\n",
    "            x_right = x_right.to(device)\n",
    "            valid_pvalid_pixels_mask_mask = valid_pixels_mask.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            pred = network(x_left, x_right)\n",
    "\n",
    "            loss, scale_losses = compute_loss(pred, gt, valid_pixels_mask)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            train_scale_losses.append(np.array([elem.item() for elem in scale_losses]))\n",
    "\n",
    "        network.train(False)\n",
    "        with torch.no_grad():\n",
    "            for x_left, x_right, gt, valid_pixels_mask in val_batch_gen:\n",
    "                x_left = x_left.to(device)\n",
    "                x_right = x_right.to(device)\n",
    "                gt = gt.to(device)\n",
    "                valid_pixels_mask = valid_pixels_mask.to(device)\n",
    "\n",
    "                pred = network(x_left, x_right)\n",
    "\n",
    "                loss, scale_losses = compute_loss(pred, gt, valid_pixels_mask)\n",
    "\n",
    "                val_loss.append(loss.item())\n",
    "                val_scale_losses.append(np.array([elem.item() for elem in scale_losses]))\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f} , \\t component loss: {}\".format(\n",
    "            np.mean(train_loss), np.mean(np.stack(train_scale_losses), axis=0)))\n",
    "        print(\"  validation loss: \\t\\t\\t{:.2f} , \\t\\t component loss: {}\".format(\n",
    "            np.mean(val_loss), np.mean(np.stack(val_scale_losses), axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f76f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_network(dispnet, opt, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49023358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(network, img_index):\n",
    "    network.train(False)\n",
    "    for i, (x_left, x_right, target, mask) in enumerate(val_batch_gen):\n",
    "        if i != img_index:\n",
    "            continue\n",
    "        pred = network(x_left.to(device), x_right.to(device))\n",
    "        pred = pred[-1].cpu()\n",
    "        break\n",
    "        \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(3,1,1)\n",
    "    plt.title('left image')\n",
    "    plt.imshow(val_loader.left_images[img_index])\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,1,2)\n",
    "    plt.title('gt')\n",
    "    plt.imshow(val_loader.targets[img_index])\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,1,3)\n",
    "    plt.title('pred')\n",
    "    plt.imshow(pred.data.numpy()[0,0])\n",
    "    plt.xticks([]), plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_result(dispnet, img_index=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0829134f-0134-4c9a-bdb3-f63da8cb8c70",
   "metadata": {},
   "source": [
    "After 20 epochs, predictions will very likely be oversmoothed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e533ae11",
   "metadata": {},
   "source": [
    "### 3.4 DispNet-Corr1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b4719",
   "metadata": {},
   "source": [
    "(image is taken from [poster](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/poster-MIFDB16.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301fd2c4",
   "metadata": {},
   "source": [
    "<img src=\"./dispnet-corr1d.png\" style=\"width:80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corr1DLayer(torch.nn.Module):\n",
    "    def __init__(self, max_disp):\n",
    "        super().__init__()\n",
    "        self.max_disp = max_disp\n",
    "\n",
    "    def forward(self, left_img, right_img):\n",
    "        corr_result = []\n",
    "        for shift in range(0, self.max_disp):\n",
    "            # YOUR CODE\n",
    "            corr = ...\n",
    "            corr_result.append(corr)\n",
    "        corr_result = torch.stack(corr_result, dim=1)\n",
    "        return corr_result\n",
    "        \n",
    "        \n",
    "class DispNetCorr1D(torch.nn.Module):\n",
    "    def __init__(self, max_disp=40):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBNRelu(3, 64, kernel_size=(7,7), stride=2, padding=(3,3))\n",
    "        self.conv2 = ConvBNRelu(64, 128, kernel_size=(5,5), stride=2, padding=(2,2))\n",
    "        \n",
    "        self.corr1d = Corr1DLayer(max_disp)\n",
    "        self.conv_refinement = ConvBNRelu(128, 64, kernel_size=(3,3), stride=1, padding=(1,1))\n",
    "        \n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            ConvBNRelu(64+max_disp, 256, kernel_size=(5,5), stride=2, padding=(2,2)),\n",
    "            ConvBNRelu(256, 256, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.conv4 = torch.nn.Sequential(\n",
    "            ConvBNRelu(256, 512, kernel_size=(3,3), stride=2, padding=(1,1)),\n",
    "            ConvBNRelu(512, 512, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.conv5 = torch.nn.Sequential(\n",
    "            ConvBNRelu(512, 512, kernel_size=(3,3), stride=2, padding=(1,1)),\n",
    "            ConvBNRelu(512, 512, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.conv6 = torch.nn.Sequential(\n",
    "            ConvBNRelu(512, 1024, kernel_size=(3,3), stride=2, padding=(1,1)),\n",
    "            ConvBNRelu(1024, 1024, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.pred6 = torch.nn.Conv2d(1024, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "        \n",
    "        self.upconv5 = UpConvBNRelu(1024, 512, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv5 = ConvBNRelu(1025, 512, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred5 = torch.nn.Conv2d(512, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "\n",
    "        self.upconv4 = UpConvBNRelu(512, 256, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv4 = ConvBNRelu(256+512+1, 256, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred4 = torch.nn.Conv2d(256, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "        \n",
    "        self.upconv3 = UpConvBNRelu(256, 128, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv3 = ConvBNRelu(128+256+1, 128, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred3 = torch.nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "\n",
    "        self.upconv2 = UpConvBNRelu(128, 64, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv2 = ConvBNRelu(64+128+1, 64, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred2 = torch.nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "\n",
    "        self.upconv1 = UpConvBNRelu(64, 32, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv1 = ConvBNRelu(32+64+1, 32, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred1 = torch.nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "        \n",
    "        self.encoder_layers = (self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.conv6)\n",
    "        self.decoder_layers = (\n",
    "            (self.upconv5, self.iconv5, self.pred5),\n",
    "            (self.upconv4, self.iconv4, self.pred4),\n",
    "            (self.upconv3, self.iconv3, self.pred3),\n",
    "            (self.upconv2, self.iconv2, self.pred2),\n",
    "            (self.upconv1, self.iconv1, self.pred1),\n",
    "        )\n",
    "\n",
    "    def forward(self, left_img, right_img):\n",
    "        # YOUR CODE\n",
    "\n",
    "        return predictions_per_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dispnet = DispNetCorr1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea31f3-c3a8-425e-89ed-bed63967f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{sum(p.numel() for p in dispnet.parameters()) / 1000_000:.1f} million parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e941c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_batch_gen:\n",
    "    left, right, target, mask = sample\n",
    "    res = dispnet(left, right)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c30a2d-3bdb-434a-81bc-1e959ccd5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "[pred.shape for pred in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c18baa6",
   "metadata": {},
   "source": [
    "### 3.5 DispNet-Corr1D training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b2703",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "dispnet = DispNetCorr1D(max_disp=40)\n",
    "dispnet = dispnet.to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(dispnet.parameters(), lr=4e-3, weight_decay=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4d78d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_network(dispnet, opt, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afad557",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_result(dispnet, img_index=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

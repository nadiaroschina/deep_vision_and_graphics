{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b10c6b-bc08-4b43-8329-ffdff391ec8e",
   "metadata": {
    "id": "76b10c6b-bc08-4b43-8329-ffdff391ec8e"
   },
   "source": [
    "# Трёхмерная реконструкция с помощью модели NeRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f83cae-008b-4005-a1b4-2d0b3f1f0974",
   "metadata": {
    "id": "98f83cae-008b-4005-a1b4-2d0b3f1f0974"
   },
   "source": [
    "План задания: реализуем простейшую модель для реконструкции трёхмерных сцен. Для этого напишем архитектуру модели, а также алгоритм отрисовки кадров. Цель задания: познакомиться с основными принципами работы современных моделей многовидовой реконструкции. Формат: дозаполнить поля с подписью \"TODO\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663fc54d-afce-4154-896a-9d1fe3b9729d",
   "metadata": {
    "id": "663fc54d-afce-4154-896a-9d1fe3b9729d"
   },
   "source": [
    "## Импортируем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2443d2cb-4a2b-4f4e-be26-95670dfd52f4",
   "metadata": {
    "id": "2443d2cb-4a2b-4f4e-be26-95670dfd52f4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.jit as jit\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae49f65-7d9f-4b6a-b1be-fc8a68947d66",
   "metadata": {
    "id": "3ae49f65-7d9f-4b6a-b1be-fc8a68947d66"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "assert device == 'cuda', \"В этом занятии нам понадобится GPU!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb17df-e3d5-4c07-acdd-0dec76cf68da",
   "metadata": {
    "id": "dddb17df-e3d5-4c07-acdd-0dec76cf68da"
   },
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10c4d7-8ba0-4858-bdd6-771a3e59d12a",
   "metadata": {
    "id": "8d10c4d7-8ba0-4858-bdd6-771a3e59d12a"
   },
   "source": [
    "Скачаем и разархивируем данные. В качестве обучаемой выборки возьмем синтетический датасет с моделью трактора из Lego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc15712-48d1-4b1e-8679-4370f815e5db",
   "metadata": {
    "id": "4fc15712-48d1-4b1e-8679-4370f815e5db"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('nerf_synthetic'):\n",
    "    !wget http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/nerf_example_data.zip\n",
    "    !unzip nerf_example_data.zip\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a00d1d0-1746-4a64-9644-9ed2037aabaf",
   "metadata": {
    "id": "8a00d1d0-1746-4a64-9644-9ed2037aabaf",
    "tags": []
   },
   "source": [
    "Вместе с картинками данные содержат параметры камеры, которые мы считаем ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9524174-f178-4fba-9a24-4ecfe71f0418",
   "metadata": {
    "id": "c9524174-f178-4fba-9a24-4ecfe71f0418"
   },
   "outputs": [],
   "source": [
    "def read_json(file_dir):\n",
    "    with open(file_dir) as fp:\n",
    "        data = json.load(fp)\n",
    "    return data\n",
    "\n",
    "def read_focal(image_width, metadata) -> float:\n",
    "    camera_angle_x = float(metadata[\"camera_angle_x\"])\n",
    "    return 0.5 * image_width / np.tan(0.5 * camera_angle_x)\n",
    "\n",
    "def read_data(base_dir, metadata, scale=1, count=None):\n",
    "    imgs, poses = [], []\n",
    "\n",
    "    if count is None:\n",
    "        count = len(metadata[\"frames\"])\n",
    "\n",
    "    for frame in tqdm(metadata[\"frames\"][:count], desc=f\"Loading Data\"):\n",
    "        img = os.path.join(base_dir, f\"{frame['file_path'][2:]}.png\")\n",
    "        img = Image.open(img)\n",
    "\n",
    "        if scale < 1.:\n",
    "            w, h = img.width, img.height\n",
    "            w = int(np.floor(scale * w))\n",
    "            h = int(np.floor(scale * h))\n",
    "            img = img.resize((w, h), Image.LANCZOS)\n",
    "\n",
    "        img = transforms.ToTensor()(img).permute(1, 2, 0)\n",
    "        img = img[:, :, :3] * img[:, :, -1:]\n",
    "        imgs.append(img)\n",
    "\n",
    "        pose = frame[\"transform_matrix\"]\n",
    "        pose = torch.FloatTensor(pose)\n",
    "        poses.append(pose)\n",
    "\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    poses = torch.stack(poses, dim=0)\n",
    "\n",
    "    return imgs, poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d0c24-2efd-42fc-bfc7-55bc634a2404",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "296d0c24-2efd-42fc-bfc7-55bc634a2404",
    "outputId": "467d7df4-3cb0-4766-82c7-26125c1090af"
   },
   "outputs": [],
   "source": [
    "metadata_train = read_json(\"./nerf_synthetic/lego/transforms_train.json\")\n",
    "metadata_val = read_json(\"./nerf_synthetic/lego/transforms_val.json\")\n",
    "metadata_test = read_json(\"./nerf_synthetic/lego/transforms_test.json\")\n",
    "\n",
    "len(metadata_train[\"frames\"]), len(metadata_val[\"frames\"]), len(metadata_test[\"frames\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8fe40f-40f8-4c40-9552-8cad1eac3346",
   "metadata": {
    "id": "fc8fe40f-40f8-4c40-9552-8cad1eac3346"
   },
   "source": [
    "### Данные для обучения\n",
    "\n",
    "В модели NeRF для того, чтобы получить итоговый цвет, мы пускаем лучи из координат камеры по напралению к объекту через каждый пиксель изображения. В каждой точке луча $r(t) = o + t \\cdot d$ модель будет предсказывать цвет и плотность объекта, что после будет агрегироваться в общий цвет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b4797-1632-469d-b4d1-03affefe5287",
   "metadata": {
    "id": "2c9b4797-1632-469d-b4d1-03affefe5287"
   },
   "outputs": [],
   "source": [
    "def get_ray_direction(H, W, focal):\n",
    "    \"\"\"\n",
    "    Get ray directions for all pixels in camera coordinate.\n",
    "    Inputs:\n",
    "        H, W, focal: image height, width and focal length\n",
    "    Outputs:\n",
    "        ray directions: (H, W, 3), the direction of the rays in camera coordinate\n",
    "    \"\"\"\n",
    "    i, j = torch.meshgrid(torch.linspace(0, W - 1, W),\n",
    "                          torch.linspace(0, H - 1, H))\n",
    "    i, j = i.t(), j.t()\n",
    "    return torch.stack(\n",
    "        [\n",
    "            (i - 0.5 * W) / focal,\n",
    "            - (j - .5 * H) / focal,\n",
    "            - torch.ones_like(i)\n",
    "        ], -1)\n",
    "\n",
    "@jit.script\n",
    "def get_rays_with_dir(directions, c2w):\n",
    "    \"\"\"\n",
    "    Get ray origin and normalized directions in world coordinate for all pixels in one image.\n",
    "    Inputs:\n",
    "        directions: (H, W, 3) precomputed ray directions in camera coordinate\n",
    "        c2w: (4, 4) transformation matrix from camera coordinate to world coordinate\n",
    "    Outputs:\n",
    "        rays_o: (H, W, 3), the origin of the rays in world coordinate\n",
    "        rays_d: (H, W, 3), the normalized direction of the rays in world coordinate\n",
    "    \"\"\"\n",
    "    # Rotate ray directions from camera coordinate to the world coordinate\n",
    "    rays_d = directions @ c2w[:3, :3].T # (H, W, 3)\n",
    "    rays_d = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "\n",
    "    # The origin of all rays is the camera origin in world coordinate\n",
    "    rays_o = c2w[:3, 3].expand(rays_d.shape) # (H, W, 3)\n",
    "    return rays_o, rays_d\n",
    "\n",
    "def build_rays(H, W, focal, poses):\n",
    "    \"\"\"\n",
    "    Generate dataset rays (origin and direction)\n",
    "    Inputs:\n",
    "        dataset (BlenderDataset): dataset context\n",
    "        H (int): frame height\n",
    "        W (int): frame width\n",
    "        focal (float): camera focal length\n",
    "        poses (Tensor): camera to world matrices (N, 4, 4)\n",
    "    Outputs:\n",
    "        ros (Tensor): ray origins (N, H, W, 3)\n",
    "        rds (Tensor): ray directions (N, H, W, 3)\n",
    "    \"\"\"\n",
    "    N = poses.size(0)\n",
    "\n",
    "    prd = get_ray_direction(H, W, focal)\n",
    "    ros = torch.zeros((N, H, W, 3), dtype=torch.float32)\n",
    "    rds = torch.zeros((N, H, W, 3), dtype=torch.float32)\n",
    "\n",
    "    c2ws = tqdm(poses, desc=f\"Building Rays\")\n",
    "    for i, c2w in enumerate(c2ws):\n",
    "        ros[i], rds[i] = get_rays_with_dir(prd, c2w)\n",
    "\n",
    "    return ros, rds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f2432-ed6d-464b-bafd-744eceeef03b",
   "metadata": {
    "id": "8f7f2432-ed6d-464b-bafd-744eceeef03b"
   },
   "outputs": [],
   "source": [
    "class NeRFDataset(Dataset):\n",
    "    def __init__(self, metadata, scale=1, data_path=\"./nerf_synthetic/lego/\", device='cpu'):\n",
    "\n",
    "        self.metadata = metadata\n",
    "\n",
    "        self.images, self.poses = read_data(data_path, self.metadata , scale=scale)\n",
    "        self.images = self.images.to(device)\n",
    "        self.poses = self.poses.to(device)\n",
    "\n",
    "        H, W = self.images.shape[1:3]\n",
    "        self.focal = read_focal(W, self.metadata)\n",
    "\n",
    "        ro, rd = build_rays(\n",
    "            H, W,\n",
    "            self.focal,\n",
    "            self.poses,\n",
    "        )\n",
    "\n",
    "        self.ro = ro\n",
    "        self.rd = rd\n",
    "\n",
    "        assert self.images.size() == self.ro.size()\n",
    "        assert self.images.size() == self.rd.size()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'image': self.images[idx],\n",
    "            'ro': self.ro[idx],\n",
    "            'rd': self.rd[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3b9b6-0f0f-4c57-975a-402af7c966f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8f3b9b6-0f0f-4c57-975a-402af7c966f2",
    "outputId": "69883f88-450b-4ec4-b432-c81cb770f306"
   },
   "outputs": [],
   "source": [
    "dataset_train = NeRFDataset(metadata_train, scale=0.125)\n",
    "dataset_val = NeRFDataset(metadata_val, scale=0.125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5460658-bb0b-4e30-bd6c-b60400868f01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "c5460658-bb0b-4e30-bd6c-b60400868f01",
    "outputId": "9bf3b66b-d03f-4cd4-9419-b79306e9bb6f"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "for i in range(4):\n",
    "    axs[i].imshow(dataset_train.images[i])\n",
    "    axs[i].axis('off')\n",
    "fig.suptitle(\"Samples from training dataset\", y=0.8, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64422be5-9cdd-4032-86c2-5881d33798fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "64422be5-9cdd-4032-86c2-5881d33798fa",
    "outputId": "f5ded238-1ca3-4d19-8652-45ec650c1cb5"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "for i in range(4):\n",
    "    axs[i].imshow(dataset_val.images[i])\n",
    "    axs[i].axis('off')\n",
    "fig.suptitle(\"Samples from validation dataset\", y=0.8, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e7234-cf5e-4299-beb8-d603fccaa757",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be3e7234-cf5e-4299-beb8-d603fccaa757",
    "outputId": "571d1d8e-3d93-4ba4-dde1-c191d2965f6d"
   },
   "outputs": [],
   "source": [
    "dataset_train.images.shape, dataset_train.ro.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9fcad8-322c-4a25-b7d0-3079b4fef656",
   "metadata": {
    "id": "fe9fcad8-322c-4a25-b7d0-3079b4fef656",
    "tags": []
   },
   "source": [
    "## Архитектура модели (0.2 + 0.3 балла)\n",
    "\n",
    "[NeRF](https://www.matthewtancik.com/nerf) представляет сцену с помощью полносвязной нейронной сети.\n",
    "\n",
    "На вход поступает точка в пространстве $x$ и направление луча $d$.\n",
    "Возвращает модель плотность $\\sigma$ и цвет $C$.\n",
    "Плотность $\\sigma$ зависит от поступивших на вход координат точки, а цвет зависит как от координат, так и от направления движения луча. Для этого архитектура разбивается на несколько блоков:\n",
    "$$\n",
    "\\begin{align}\n",
    "h_1(x) &= \\text{block}_1(x) \\\\\n",
    "h_2(x), \\sigma(x) &= \\text{block}_2(x, h_1) \\\\\n",
    "c(x, d) &= \\text{block}_3(h_2, d)\n",
    "\\end{align},\n",
    "$$\n",
    "где каждый блок представляет из себя полносвязную нейронную сеть.\n",
    "Каждый блок мог бы принимать на вход координаты в явном виде, однако более успешным оказывается подход с использованием позиционного кодирования входов сети.\n",
    "Опишем его подробнее\n",
    "\n",
    "Интуитивно, модель трёхмерной сцены должна допускать резкие изменения цвета и плотности в зависимости от $x$. Однако, [оказывается](https://arxiv.org/abs/2006.10739), что полносязная сеть требует долгого обучения для того чтобы добиться этого эффекта. В качестве обходного решения, в модели NeRF было предложено использовать позиционное кодирование входа:\n",
    "$$\n",
    "\\gamma_L(x) = [x, \\cos(2^0 \\pi x), \\sin(2^0 \\pi x), \\dots, \\cos(2^{L - 1} \\pi x), \\sin(2^{L - 1} \\pi x)].\n",
    "$$\n",
    "Выше приведена формула для скалярного $x$, а для векторного входа $x$ кодирование применяется к каждой координате по отдельности.\n",
    "\n",
    "В итоге архитектура имеет следующий вид:\n",
    "<img src=\"nerf_architecture.png\" width=\"750\">\n",
    "\n",
    "Ниже вам надо реализовать функцию для позиционного кодирования и прямого прохода в классе для модели NeRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c887a00b-78cc-4660-a6a8-c799458a40ff",
   "metadata": {
    "id": "c887a00b-78cc-4660-a6a8-c799458a40ff"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(x, L, include_input=True):\n",
    "    # TODO (0.2 балла за реализацию)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bbeaf5-4cc5-40cc-9af7-eeb65aca15d0",
   "metadata": {
    "id": "e4bbeaf5-4cc5-40cc-9af7-eeb65aca15d0"
   },
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_freq_x=10, num_freq_d=4, include_input=True):\n",
    "        super(NeRF, self).__init__()\n",
    "\n",
    "        self.num_freq_x = num_freq_x\n",
    "        self.num_freq_d = num_freq_d\n",
    "        self.include_input = include_input\n",
    "        input_dim1 = (num_freq_x * 6 + 3) if self.include_input else (num_freq_x * 6)\n",
    "        input_dim2 = (num_freq_d * 6 + 3) if self.include_input else (num_freq_d * 6) # direction vector is an unit vector\n",
    "\n",
    "        self.block1 =  nn.Sequential(\n",
    "            nn.Linear(input_dim1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(input_dim1 + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim + 1), # add one number for density\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(input_dim2 + hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, points, directions):\n",
    "        # points, directions: [batch_size, 3]\n",
    "        # TODO: (0.3 балла за реализацию)\n",
    "        pass\n",
    "        #return c, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ad3a3-7233-4d4b-88b4-13f661cad8d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "897ad3a3-7233-4d4b-88b4-13f661cad8d6",
    "outputId": "38333915-d3bc-4851-d0d3-a31e27a8ce15"
   },
   "outputs": [],
   "source": [
    "a = NeRF(include_input=False)\n",
    "\n",
    "p = torch.rand(10, 3)\n",
    "d = torch.rand(10, 3)\n",
    "\n",
    "a(p, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf24d19-b85d-4b8d-9baa-893f2d0215ab",
   "metadata": {
    "id": "7cf24d19-b85d-4b8d-9baa-893f2d0215ab"
   },
   "source": [
    "### Отрисовка лучей (0.5 балла)\n",
    "\n",
    "После вычисления плотности и цвета вдоль луча мы вычисляет цвет пикселя на изображении, аггрегируя цвет и плотность с помощью алгоритма $\\alpha$-compositing.\n",
    "\n",
    "Для $N$ точек $\\{x_i\\}_{i=1^N}$ луча $r$ введем обозначения $\\sigma_i = \\sigma(x_i)$ и $c_i = c(x_i, d)$, где $d$ обозначает направление луча.\n",
    "Итоговый цвет $C(r)$ может быть вычислен по формуле (3) из оригинальной [статьи](https://arxiv.org/abs/2003.08934)\n",
    "$$\n",
    "C(r) = \\sum_{i=1}^N \\alpha_i \\left( \\prod_{j < i} (1 - \\alpha_j) \\right) c_i\n",
    "$$\n",
    "для непрозрачности $\\alpha_i = 1 - \\exp(-\\sigma_i \\delta_i)$ и ширины разбиения луча $\\delta_i$ (можно считать $\\delta_i = \\tfrac{1}{N}$).\n",
    "\n",
    "Формула перевзвешивает величины $c_i$, отдавая приоритет непрозрачным точкам встреченным для ранних индексов $i$.\n",
    "Физическая интерпретация алгоритма описана в [этой](https://courses.cs.duke.edu/spring03/cps296.8/papers/max95opticalModelsForDirectVolumeRendering.pdf) статье.\n",
    "Вес $w_i = \\alpha_i \\prod_{j < i} (1 - \\alpha_i)$ соответствует вероятности события, при котором выпущенная с вероятностью $\\alpha_i$ из точки $i$ частица света пройдет через точки $j < i$ с вероятностью прохождения $(1 - \\alpha_j)$ и долетит до камеры.\n",
    "Цвет $C(r)$, в свою очередь, соответствует среднему цвету пришедших частиц.\n",
    "\n",
    "Ниже вам надо реализовать вычисление цвета пикселя по описанному выше алгоритму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a874d67-c32e-4418-8be5-f3eceac41257",
   "metadata": {
    "id": "1a874d67-c32e-4418-8be5-f3eceac41257"
   },
   "outputs": [],
   "source": [
    "def get_ray_samples(ray_origins, hn, hf, nb_bins, device):\n",
    "    t = torch.linspace(hn, hf, nb_bins, device=device).expand(ray_origins.shape[0], nb_bins)\n",
    "\n",
    "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "    lower = torch.cat((t[:, :1], mid), -1)\n",
    "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "    u = torch.rand(t.shape, device=device)\n",
    "\n",
    "    t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "    delta = torch.cat((t[:, 1:] - t[:, :-1], torch.tensor([1e10], device=device).expand(ray_origins.shape[0], 1)), -1)\n",
    "    return t, delta\n",
    "\n",
    "def render_rays(nerf_model, ray_origins, ray_directions, hn=0, hf=0.5, nb_bins=192):\n",
    "\n",
    "    device = ray_origins.device\n",
    "\n",
    "    t, delta = get_ray_samples(ray_origins, hn, hf, nb_bins, device)\n",
    "\n",
    "    # Compute the 3D points along each ray\n",
    "    x = ray_origins.unsqueeze(1) + t.unsqueeze(2) * ray_directions.unsqueeze(1)   # [batch_size, nb_bins, 3]\n",
    "    ray_directions = ray_directions.expand(nb_bins, ray_directions.shape[0], 3).transpose(0, 1)\n",
    "    \n",
    "    colors, sigma = nerf_model(x.reshape(-1, 3), ray_directions.reshape(-1, 3))\n",
    "    # TODO: вычислить цвет пикселя C на основе полученных выше переменных (0.5 балла)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4aee64-2d12-4a38-a609-b62e654635d3",
   "metadata": {
    "id": "2e4aee64-2d12-4a38-a609-b62e654635d3"
   },
   "source": [
    "## Обучение\n",
    "\n",
    "Давайте обучим модель, используя реализованные выше компоненты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfd6bd-0d27-4c63-9ff5-bd43c909e7fa",
   "metadata": {
    "id": "d3cfd6bd-0d27-4c63-9ff5-bd43c909e7fa"
   },
   "outputs": [],
   "source": [
    "def psnr(img_gd, img_gen):\n",
    "    mse = ((img_gd - img_gen) ** 2).mean()\n",
    "    mse = torch.clamp(mse, min=1e-10)\n",
    "    return -10.0 * torch.log10(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e047aa-1426-4300-a11d-8ae2ac214e06",
   "metadata": {
    "id": "01e047aa-1426-4300-a11d-8ae2ac214e06"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, train_loader, val_img, device='cuda', hn=0, hf=1, nb_epochs=int(1e5), nb_bins=192, chunk_size=1024*2):\n",
    "    all_training_loss = []\n",
    "    psnr_values = []\n",
    "    psnr_values_val = []\n",
    "    psnr_values_train = []\n",
    "\n",
    "    for epoch in tqdm(range(nb_epochs), desc=\"Epochs\"):\n",
    "        epoch_loss = []\n",
    "        psnr_train_epoch = []\n",
    "        psnr_val_epoch = []\n",
    "\n",
    "        for j, batch in tqdm(enumerate(train_loader), total=len(train_loader), leave=False, desc=\"Images\"):\n",
    "            model.train()\n",
    "            ray_origins = batch['ro'].view(-1, 3)\n",
    "            ray_directions = batch['rd'].view(-1, 3)\n",
    "            image_gd = batch['image'].view(-1, 3)\n",
    "\n",
    "            chunk_losses, rendered_chunks = [], []\n",
    "\n",
    "            for i in tqdm(range(0, ray_origins.shape[0], chunk_size), leave=False, desc=\"Chunks\"):\n",
    "                ro_chunk = ray_origins[i:i+chunk_size].to(device)\n",
    "                rd_chunk = ray_directions[i:i+chunk_size].to(device)\n",
    "                image_gd_chunk = image_gd[i:i+chunk_size].to(device)\n",
    "\n",
    "                rgb_map_chunk = render_rays(model, ro_chunk, rd_chunk, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "\n",
    "                loss = ((image_gd_chunk - rgb_map_chunk) ** 2).mean()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                rendered_chunks.append(rgb_map_chunk.cpu().detach())\n",
    "                chunk_losses.append(loss.item())\n",
    "\n",
    "                del image_gd_chunk, loss, ro_chunk, rd_chunk, rgb_map_chunk\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            image_gen = torch.cat(rendered_chunks, dim=0)\n",
    "            train_psnr = psnr(image_gd.cpu().detach(), image_gen)\n",
    "            psnr_train_epoch.append(train_psnr)\n",
    "\n",
    "            epoch_loss.append(np.mean(chunk_losses))\n",
    "\n",
    "            # Validation step\n",
    "            if j % 5 == 0:\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    ray_origins_val = val_img['ro'].view(-1, 3)\n",
    "                    ray_directions_val = val_img['rd'].view(-1, 3)\n",
    "                    image_gd_val = val_img['image'].view(-1, 3)\n",
    "                    rendered_chunks_val = []\n",
    "\n",
    "                    for i in range(0, ray_origins_val.shape[0], chunk_size):\n",
    "                        ro_chunk_val = ray_origins_val[i:i+chunk_size].to(device)\n",
    "                        rd_chunk_val = ray_directions_val[i:i+chunk_size].to(device)\n",
    "\n",
    "                        # Render validation rays\n",
    "                        rgb_map_chunk_val = render_rays(model, ro_chunk_val, rd_chunk_val, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "                        rendered_chunks_val.append(rgb_map_chunk_val.cpu().detach())\n",
    "\n",
    "                    # Concatenate validation chunks\n",
    "                    image_gen_val = torch.cat(rendered_chunks_val, dim=0)\n",
    "                    val_psnr = psnr(image_gd_val.cpu().detach(), image_gen_val)\n",
    "                    psnr_val_epoch.append(val_psnr)\n",
    "\n",
    "                    clear_output(wait=True)\n",
    "                    plt.figure(figsize=(15, 9))\n",
    "                    plt.subplot(2, 3, 1)\n",
    "                    plt.imshow(image_gd.view(*batch['image'].shape[1:]).cpu().detach().numpy())\n",
    "                    plt.title(\"Ground Truth Image (Train)\")\n",
    "\n",
    "                    plt.subplot(2, 3, 2)\n",
    "                    plt.imshow(image_gen.view(*batch['image'].shape[1:]).cpu().detach().numpy())\n",
    "                    plt.title(\"Generated Image (Train)\")\n",
    "\n",
    "                    plt.subplot(2, 3, 3)\n",
    "                    plt.plot(range(len(psnr_train_epoch)), psnr_train_epoch)\n",
    "                    plt.title(f\"Train PSNR\")\n",
    "\n",
    "                    plt.subplot(2, 3, 4)\n",
    "                    plt.imshow(image_gd_val.view(*val_img['image'].shape).cpu().detach().numpy())\n",
    "                    plt.title(\"Ground Truth Image (Val)\")\n",
    "\n",
    "                    plt.subplot(2, 3, 5)\n",
    "                    plt.imshow(image_gen_val.view(*val_img['image'].shape).cpu().detach().numpy())\n",
    "                    plt.title(\"Generated Image (Val)\")\n",
    "\n",
    "\n",
    "                    plt.subplot(2, 3, 6)\n",
    "                    plt.plot(range(len(psnr_val_epoch)), psnr_val_epoch)\n",
    "                    plt.title(\"Val PSNR\")\n",
    "\n",
    "                    plt.show()\n",
    "\n",
    "        all_training_loss.append(np.mean(epoch_loss))\n",
    "        psnr_values_train.append(np.mean(psnr_train_epoch))\n",
    "        psnr_values_val.append(np.mean(psnr_val_epoch))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    return all_training_loss, psnr_values_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef33637-e9d0-4067-966b-6d16f7ce9bdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "5ef33637-e9d0-4067-966b-6d16f7ce9bdb",
    "outputId": "8cfdb36a-18ab-4443-e2d2-e0c4d3bd7d53"
   },
   "outputs": [],
   "source": [
    "model = NeRF(hidden_dim=256).to(device)\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(model_optimizer, milestones=[2, 4, 8], gamma=0.5)\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=1)\n",
    "val_img = dataset_val[5]\n",
    "\n",
    "train(model, model_optimizer, scheduler, train_loader, val_img, nb_epochs=50, device=device, hn=2, hf=6, nb_bins=192, chunk_size=13*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228306d6",
   "metadata": {},
   "source": [
    "Удалось ли сгенерировать правдоподобные реконструкции модели с помощью выученной сети?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

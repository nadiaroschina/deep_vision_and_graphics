{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "wsy0elzssgh1p1m5medy3t"
   },
   "source": [
    "# Generative Models Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "backend = 'Colab'\n",
    "if backend == 'Colab':\n",
    "    !git clone https://github.com/yandexdataschool/deep_vision_and_graphics.git\n",
    "    !sudo apt install -y ninja-build\n",
    "    %cd /content/deep_vision_and_graphics/week07-latent_models\n",
    "    !wget https://www.dropbox.com/s/2kpsomtla61gjrn/pretrained.tar\n",
    "    !tar -xvf pretrained.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToPILImage, CenterCrop, ToTensor, Resize, GaussianBlur\n",
    "from gans.gan_load import make_stylegan2, make_big_gan\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "\n",
    "def to_image(tensor, adaptive=False):\n",
    "    if len(tensor.shape) == 4:\n",
    "        tensor = tensor[0]\n",
    "    if adaptive:\n",
    "        tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "    else:\n",
    "        tensor = ((tensor + 1) / 2).clamp(0, 1)\n",
    "    return ToPILImage()((255 * tensor.cpu().detach()).to(torch.uint8))\n",
    "\n",
    "\n",
    "def to_image_grid(tensor, adaptive=False, **kwargs):\n",
    "    return to_image(make_grid(tensor, **kwargs), adaptive)\n",
    "\n",
    "\n",
    "def to_image_grid(tensor, adaptive=False, **kwargs):\n",
    "    return to_image(make_grid(tensor, **kwargs), adaptive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "noh6b9qig2ed2vi7cwdp6"
   },
   "source": [
    "## LPIPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "t75vnt8rrac4aniq6pz"
   },
   "source": [
    "https://richzhang.github.io/PerceptualSimilarity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "import lpips\n",
    "lpips_dist = lpips.LPIPS('alexnet').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "img = CenterCrop(256)(Resize(256)(Image.open('sample.png')))\n",
    "ref_img = ToTensor()(img)[:3]\n",
    "normalize = lambda x: 2 * x.unsqueeze(0) - 1\n",
    "\n",
    "img_blured = normalize(ToTensor()(GaussianBlur(5, sigma=(2, 2))(img))[:3]).cuda()\n",
    "ref_img = normalize(ref_img.cuda())\n",
    "img_noised = ref_img + 0.3 * torch.randn_like(ref_img).cuda()\n",
    "\n",
    "\n",
    "_, axs = plt.subplots(1, 3, dpi=250)\n",
    "for ax in axs: ax.axis('off')\n",
    "    \n",
    "axs[0].imshow(to_image(img_blured))\n",
    "axs[1].imshow(to_image(ref_img))\n",
    "axs[2].imshow(to_image(img_noised))\n",
    "\n",
    "axs[0].set_title(f'{lpips_dist(img_blured, ref_img).item(): 0.2f}');\n",
    "axs[2].set_title(f'{lpips_dist(img_noised, ref_img).item(): 0.2f}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "4t7hodga2ylswyr42nvv0q"
   },
   "source": [
    "## Deep Image Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "780c1s04nt86f16s79vh65"
   },
   "source": [
    "https://dmitryulyanov.github.io/deep_image_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "from resnet18_gen import make_resnet_generator, RES_GEN_CONFIGS\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "G_ref = make_resnet_generator(RES_GEN_CONFIGS[256])\n",
    "G_ref.cuda().eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn([1, G_ref.dim_z], device='cuda')\n",
    "    img = G_ref(z)\n",
    "    plt.imshow(to_image_grid(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def optimize_deep_image_prior(ref_img, lpips_weight: float, mask: torch.Tensor):\n",
    "    n_steps = 2000\n",
    "\n",
    "    G = deepcopy(G_ref)\n",
    "    G.cuda().train()\n",
    "\n",
    "    opt = torch.optim.Adam(G.parameters())\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    for step in trange(n_steps):\n",
    "        G.zero_grad()\n",
    "        rec = G(z)\n",
    "\n",
    "        rec_loss = (1.0 - lpips_weight) * mse(mask * rec, mask * ref_img) + \\\n",
    "                    lpips_weight * lpips_dist(mask * rec, mask * ref_img)\n",
    "        rec_loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    return rec, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "_, axs = plt.subplots(1, 2, dpi=150)\n",
    "for ax in axs: ax.axis('off')\n",
    "\n",
    "mask = torch.ones_like(ref_img)\n",
    "mask[:, :, 100: 150, 100: 150] = 0.0\n",
    "rec, G = optimize_deep_image_prior(ref_img, 0.0, mask)\n",
    " \n",
    "plt.title('lpips-weight: 0.0')\n",
    "axs[0].imshow(to_image(mask * rec, True))\n",
    "axs[1].imshow(to_image(rec, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "_, axs = plt.subplots(1, 2, dpi=150)\n",
    "for ax in axs: ax.axis('off')\n",
    "\n",
    "mask = torch.ones_like(ref_img)\n",
    "mask[:, :, 100: 150, 100: 150] = 0.0\n",
    "rec, G = optimize_deep_image_prior(ref_img, 0.5, mask)\n",
    " \n",
    "plt.title('lpips-weight: 0.5')\n",
    "axs[0].imshow(to_image(mask * rec, True))\n",
    "axs[1].imshow(to_image(rec, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "def activations_save_hook(self, input, out):\n",
    "    setattr(self, 'activations', out)\n",
    "    return out\n",
    "\n",
    "hooks = []\n",
    "for i in range(len(G)):\n",
    "    hooks.append(G[i].register_forward_hook(activations_save_hook))\n",
    "\n",
    "with torch.no_grad():\n",
    "    G(z)\n",
    "\n",
    "for h in hooks:\n",
    "    h.remove()\n",
    "\n",
    "# plot activations\n",
    "for i, axs in enumerate(plt.subplots(len(G), 1, dpi=250)[1]):\n",
    "    axs.axis('off')\n",
    "    axs.text(0, -0.7, f'layer {i}', fontdict=dict(size=4, color='red'))\n",
    "    axs.imshow(to_image_grid(G[i].activations[0, :8].unsqueeze(1), True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "plt.figure(dpi=300)\n",
    "plt.subplot(121)\n",
    "plt.axis('off')\n",
    "plt.imshow(Image.open('stylegan2_activations.png'))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.axis('off')\n",
    "plt.imshow(Image.open('stylegan2_sample.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "from scipy.stats import truncnorm\n",
    "G = make_big_gan('pretrained/G_ema.pth', 128).cuda().eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn([2, 512]).cuda()\n",
    "\n",
    "    classes = nn.Parameter(torch.tensor([12, 200], dtype=torch.int64), requires_grad=False).cuda()\n",
    "    cl_embed = G.big_gan.shared(classes)\n",
    "    cl_embed_swap = torch.stack([cl_embed[1], cl_embed[0]])\n",
    "\n",
    "    imgs = G.big_gan(z, cl_embed)\n",
    "    imgs_cl_swap = G.big_gan(z, cl_embed_swap)\n",
    "\n",
    "    interps = torch.arange(0, 1.01, 0.2).cuda()\n",
    "    embeds = torch.stack([torch.lerp(cl_embed[0], cl_embed[1], a) for a in interps])\n",
    "\n",
    "    imgs_cl_interp = G.big_gan(z[0][None].repeat(len(embeds), 1), embeds)\n",
    "    \n",
    "_, axs = plt.subplots(3, 1, dpi=200)\n",
    "for ax in axs: ax.axis('off')\n",
    "\n",
    "axs[0].imshow(to_image_grid(imgs))\n",
    "axs[1].imshow(to_image_grid(imgs_cl_swap))\n",
    "axs[2].imshow(to_image_grid(imgs_cl_interp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "# truncation\n",
    "\n",
    "batch = 16\n",
    "truncation = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    tr = truncnorm(-truncation, truncation)\n",
    "    z = torch.from_numpy(tr.rvs(batch * 512)).view([batch, 512]).float().cuda()\n",
    "\n",
    "    cl_embed = G.big_gan.shared(torch.tensor(batch * [239], dtype=torch.int64).cuda())\n",
    "    imgs = G.big_gan(z, cl_embed)\n",
    "\n",
    "to_image_grid(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unconditional Generaion and Latent Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('stylegan.png').resize([520, 600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "G = make_stylegan2(resolution=1024,\n",
    "                   weights='pretrained/stylegan2-ffhq-config-f.pt').eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn([4, 512]).cuda()\n",
    "    imgs = G(z)\n",
    "\n",
    "    z = torch.stack([a * z[0] + (1 - a) * z[1] for a in torch.arange(0, 1.01, 0.15)])\n",
    "    imgs_interp = G(z)\n",
    "\n",
    "_, axs = plt.subplots(2, 1, dpi=200)\n",
    "for ax in axs: ax.axis('off')\n",
    "\n",
    "axs[0].imshow(to_image_grid(imgs, nrow=6))\n",
    "axs[1].imshow(to_image_grid(imgs_interp, nrow=len(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('infogan.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "from supervised_deformation_finder.model import CelebaAttributeClassifier\n",
    "from supervised_deformation_finder.utils import \\\n",
    "    prepare_generator_output_for_celeba_regressor as preprocess\n",
    "\n",
    "\n",
    "print('- Attributes -\\n')\n",
    "with open('supervised_deformation_finder/celeba_attributes.txt') as f:\n",
    "    attributes = f.readline().split(' ')\n",
    "    attributes.sort()\n",
    "    for i, att in enumerate(attributes):\n",
    "        if i % 4 != 0:\n",
    "            print(att.ljust(22), end='')\n",
    "        else:\n",
    "            print(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "from dataclasses import dataclass\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ShiftedGSample:\n",
    "    latent: list = None\n",
    "    val: float = None\n",
    "\n",
    "\n",
    "def train_normal(samples, svm_max_iter=10_000):\n",
    "    num_samples = len(samples)\n",
    "    latents = torch.stack([s.latent for s in samples])\n",
    "    expectations = torch.tensor([s.val for s in samples])\n",
    "\n",
    "    # estimate attribute direction\n",
    "    print(f'Starting SVM training with {num_samples} samples')\n",
    "    svm = LinearSVR(max_iter=svm_max_iter)\n",
    "    svm.fit(latents, expectations)\n",
    "    normal = torch.from_numpy(svm.coef_).to(torch.float).cuda()\n",
    "\n",
    "    return normal\n",
    "\n",
    "\n",
    "# accumulate statistics\n",
    "\n",
    "regressor = CelebaAttributeClassifier('Smiling', 'pretrained/regressor.pth').cuda().eval()\n",
    "\n",
    "num_steps, batch = 200, 8\n",
    "samples = []\n",
    "for latents in tqdm(torch.randn([num_steps, batch, 512])):\n",
    "    with torch.no_grad():\n",
    "        latents = G.style_gan2.style(latents.cuda())\n",
    "        imgs = G(latents, w_space=True)\n",
    "        probs = regressor.get_probs(preprocess(imgs))[:, 1]\n",
    "    samples += [ShiftedGSample(l, p) for l, p in zip(latents.cpu(), probs.cpu())]\n",
    "\n",
    "for i, ax in enumerate(plt.subplots(1, len(imgs), dpi=250)[1]):\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'{probs[i].item(): 0.2f}', fontdict=dict(size=7))\n",
    "    ax.imshow(to_image(imgs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "shift = train_normal(samples, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L\n",
    "with torch.no_grad():\n",
    "    z = torch.randn([8, 512]).cuda()\n",
    "    imgs_orig = G(z)\n",
    "    \n",
    "    w = G.style_gan2.style(z)\n",
    "    imgs_shifted = G(w + 5 * shift, w_space=True)\n",
    "\n",
    "plt.figure(dpi=250)\n",
    "plt.axis('off')\n",
    "plt.imshow(to_image_grid(torch.cat([imgs_orig, imgs_shifted]), nrow=len(imgs_orig)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

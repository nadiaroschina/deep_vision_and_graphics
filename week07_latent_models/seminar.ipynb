{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Генеративные модели: Семинар 1 - Латентные пространства\n",
                "\n",
                "## Цели семинара\n",
                "- Изучить различные метрики качества генерации (LPIPS)\n",
                "- Познакомиться с Deep Image Prior\n",
                "- Освоить условную генерацию и манипуляции в латентном пространстве\n",
                "\n",
                "## Предварительные требования\n",
                "- PyTorch\n",
                "- LPIPS\n",
                "- torchvision\n",
                "- PIL\n",
                "- matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import logging\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "from tqdm.auto import tqdm, trange\n",
                "from matplotlib import pyplot as plt\n",
                "from IPython.display import display, HTML\n",
                "import ipywidgets as widgets\n",
                "\n",
                "from PIL import Image\n",
                "from torchvision.utils import make_grid\n",
                "from torchvision.transforms import ToPILImage, CenterCrop, ToTensor, Resize, GaussianBlur\n",
                "\n",
                "# Настройка логирования\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "# Устанавливаем сид для воспроизводимости\n",
                "RANDOM_SEED = 42\n",
                "torch.manual_seed(RANDOM_SEED)\n",
                "np.random.seed(RANDOM_SEED)\n",
                "\n",
                "# Конфигурация\n",
                "CONFIG = {\n",
                "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
                "    'batch_size': 8,\n",
                "    'n_steps': 2000,\n",
                "    'truncation': 10,\n",
                "    'lpips_weight': 0.5,\n",
                "    'save_dir': Path('results')\n",
                "}\n",
                "\n",
                "# Создаем директорию для результатов\n",
                "CONFIG['save_dir'].mkdir(exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def setup_environment(backend='Colab'):\n",
                "    \"\"\"Настройка окружения и загрузка необходимых файлов.\n",
                "    \n",
                "    Args:\n",
                "        backend (str): Тип окружения ('Colab' или 'Local')\n",
                "    \"\"\"\n",
                "    try:\n",
                "        if backend == 'Colab':\n",
                "            !git clone https://github.com/yandexdataschool/deep_vision_and_graphics.git\n",
                "            !sudo apt install -y ninja-build\n",
                "            %cd /content/deep_vision_and_graphics/week09_gans\n",
                "            !wget https://www.dropbox.com/s/2kpsomtla61gjrn/pretrained.tar\n",
                "            !tar -xvf pretrained.tar\n",
                "        logger.info(f'Successfully set up {backend} environment')\n",
                "    except Exception as e:\n",
                "        logger.error(f'Failed to setup environment: {str(e)}')\n",
                "        raise\n",
                "\n",
                "setup_environment()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Метрики качества генерации\n",
                "\n",
                "### 1.1 LPIPS (Learned Perceptual Image Patch Similarity)\n",
                "\n",
                "LPIPS - метрика, которая использует особенности, извлеченные предобученной нейронной сетью, для оценки перцептивного сходства между изображениями. В отличие от традиционных метрик (MSE, PSNR), LPIPS лучше коррелирует с человеческим восприятием качества изображений."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ImageProcessor:\n",
                "    \"\"\"Класс для обработки и визуализации изображений.\"\"\"\n",
                "    \n",
                "    def __init__(self, device=CONFIG['device']):\n",
                "        self.device = device\n",
                "        self.lpips_model = lpips.LPIPS('alexnet').to(device)\n",
                "        \n",
                "    def load_and_preprocess(self, image_path, size=256):\n",
                "        \"\"\"Загрузка и предобработка изображения.\n",
                "        \n",
                "        Args:\n",
                "            image_path (str): Путь к изображению\n",
                "            size (int): Размер выходного изображения\n",
                "            \n",
                "        Returns:\n",
                "            torch.Tensor: Нормализованный тензор изображения\n",
                "        \"\"\"\n",
                "        try:\n",
                "            img = CenterCrop(size)(Resize(size)(Image.open(image_path)))\n",
                "            tensor = ToTensor()(img)[:3]\n",
                "            return 2 * tensor.unsqueeze(0).to(self.device) - 1\n",
                "        except Exception as e:\n",
                "            logger.error(f'Failed to load image: {str(e)}')\n",
                "            raise\n",
                "            \n",
                "    def compare_images(self, ref_img, modified_images, titles=None):\n",
                "        \"\"\"Сравнение референсного изображения с модифицированными версиями.\n",
                "        \n",
                "        Args:\n",
                "            ref_img (torch.Tensor): Референсное изображение\n",
                "            modified_images (list): Список модифицированных изображений\n",
                "            titles (list): Список заголовков для изображений\n",
                "        \"\"\"\n",
                "        n_images = len(modified_images) + 1\n",
                "        _, axs = plt.subplots(1, n_images, figsize=(4*n_images, 4), dpi=100)\n",
                "        \n",
                "        for ax in axs:\n",
                "            ax.axis('off')\n",
                "        \n",
                "        # Показываем референсное изображение\n",
                "        axs[0].imshow(to_image(ref_img))\n",
                "        axs[0].set_title('Reference')\n",
                "        \n",
                "        # Показываем модифицированные изображения с LPIPS метрикой\n",
                "        for i, img in enumerate(modified_images, 1):\n",
                "            lpips_score = self.lpips_model(img, ref_img).item()\n",
                "            axs[i].imshow(to_image(img))\n",
                "            title = f'{titles[i-1]}\\nLPIPS: {lpips_score:.3f}' if titles else f'LPIPS: {lpips_score:.3f}'\n",
                "            axs[i].set_title(title)\n",
                "            \n",
                "        plt.tight_layout()\n",
                "\n",
                "# Создаем интерактивные виджеты для экспериментов с искажениями\n",
                "def create_distortion_widgets():\n",
                "    blur_sigma = widgets.FloatSlider(value=2.0, min=0.1, max=5.0, step=0.1, description='Blur σ:')\n",
                "    noise_std = widgets.FloatSlider(value=0.3, min=0.1, max=1.0, step=0.1, description='Noise σ:')\n",
                "    return blur_sigma, noise_std\n",
                "\n",
                "processor = ImageProcessor()\n",
                "blur_sigma, noise_std = create_distortion_widgets()\n",
                "\n",
                "def update_comparison(blur_sigma, noise_std):\n",
                "    ref_img = processor.load_and_preprocess('sample.png')\n",
                "    \n",
                "    # Создаем искаженные версии\n",
                "    img_blured = processor.normalize(ToTensor()(GaussianBlur(5, sigma=(blur_sigma, blur_sigma))(img))[:3])\n",
                "    img_noised = ref_img + noise_std * torch.randn_like(ref_img)\n",
                "    \n",
                "    processor.compare_images(ref_img, [img_blured, img_noised], ['Blurred', 'Noised'])\n",
                "\n",
                "widgets.interactive(update_comparison, blur_sigma=blur_sigma, noise_std=noise_std)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Deep Image Prior\n",
                "\n",
                "Deep Image Prior (DIP) - это техника, которая использует структуру нейронной сети как prior для задач обработки изображений. В отличие от классических методов, DIP не требует предварительного обучения на большом наборе данных."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DeepImagePrior:\n",
                "    \"\"\"Реализация Deep Image Prior с визуализацией процесса оптимизации.\"\"\"\n",
                "    \n",
                "    def __init__(self, config=CONFIG):\n",
                "        self.config = config\n",
                "        self.device = config['device']\n",
                "        self.results = []\n",
                "        \n",
                "    def optimize(self, ref_img, lpips_weight, mask, callback=None):\n",
                "        \"\"\"Оптимизация генеративной модели.\n",
                "        \n",
                "        Args:\n",
                "            ref_img (torch.Tensor): Референсное изображение\n",
                "            lpips_weight (float): Вес LPIPS loss\n",
                "            mask (torch.Tensor): Маска для оптимизации\n",
                "            callback (callable): Функция обратного вызова для визуализации\n",
                "            \n",
                "        Returns:\n",
                "            tuple: (оптимизированное изображение, модель)\n",
                "        \"\"\"\n",
                "        G = deepcopy(G_ref)\n",
                "        G.to(self.device).train()\n",
                "        \n",
                "        optimizer = torch.optim.Adam(G.parameters())\n",
                "        mse = nn.MSELoss()\n",
                "        lpips_model = lpips.LPIPS('alexnet').to(self.device)\n",
                "        \n",
                "        # Progress bar с метриками\n",
                "        pbar = tqdm(range(self.config['n_steps']))\n",
                "        for step in pbar:\n",
                "            G.zero_grad()\n",
                "            rec = G(z)\n",
                "            \n",
                "            # Вычисляем потери\n",
                "            mse_loss = mse(mask * rec, mask * ref_img)\n",
                "            lpips_loss = lpips_model(mask * rec, mask * ref_img)\n",
                "            total_loss = (1.0 - lpips_weight) * mse_loss + lpips_weight * lpips_loss\n",
                "            \n",
                "            total_loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            # Обновляем progress bar\n",
                "            pbar.set_description(f'MSE: {mse_loss.item():.4f}, LPIPS: {lpips_loss.item():.4f}')\n",
                "            \n",
                "            # Сохраняем промежуточные результаты\n",
                "            if step % 100 == 0:\n",
                "                self.results.append({\n",
                "                    'step': step,\n",
                "                    'image': rec.detach().cpu(),\n",
                "                    'mse_loss': mse_loss.item(),\n",
                "                    'lpips_loss': lpips_loss.item()\n",
                "                })\n",
                "                if callback:\n",
                "                    callback(self.results[-1])\n",
                "                    \n",
                "        return rec, G\n",
                "    \n",
                "    def visualize_progress(self):\n",
                "        \"\"\"Визуализация процесса оптимизации.\"\"\"\n",
                "        n_samples = len(self.results)\n",
                "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
                "        \n",
                "        # График потерь\n",
                "        mse_losses = [r['mse_loss'] for r in self.results]\n",
                "        lpips_losses = [r['lpips_loss'] for r in self.results]\n",
                "        \n",
                "        ax1.plot(steps, mse_losses, 'b-', label='MSE Loss')\n",
                "        ax1.plot(steps, lpips_losses, 'r-', label='LPIPS Loss')\n",
                "        ax1.set_xlabel('Steps')\n",
                "        ax1.set_ylabel('Loss')\n",
                "        ax1.legend()\n",
                "        ax1.grid(True)\n",
                "        \n",
                "        # Визуализация изображений\n",
                "        selected_steps = np.linspace(0, len(self.results)-1, 5).astype(int)\n",
                "        images = [self.results[i]['image'] for i in selected_steps]\n",
                "        ax2.imshow(to_image_grid(torch.cat(images), nrow=len(images)))\n",
                "        ax2.axis('off')\n",
                "        ax2.set_title('Progression of Image Generation')\n",
                "        \n",
                "        plt.tight_layout()\n",
                "        return fig\n",
                "\n",
                "\n",
                "# Создаем интерактивный интерфейс для Deep Image Prior\n",
                "def create_dip_interface():\n",
                "    lpips_weight = widgets.FloatSlider(\n",
                "        value=0.5,\n",
                "        min=0.0,\n",
                "        max=1.0,\n",
                "        step=0.1,\n",
                "        description='LPIPS Weight:'\n",
                "    )\n",
                "    \n",
                "    mask_size = widgets.IntRangeSlider(\n",
                "        value=[100, 150],\n",
                "        min=0,\n",
                "        max=256,\n",
                "        step=10,\n",
                "        description='Mask Range:'\n",
                "    )\n",
                "    \n",
                "    run_button = widgets.Button(description='Run Optimization')\n",
                "    output = widgets.Output()\n",
                "    \n",
                "    return lpips_weight, mask_size, run_button, output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Демонстрация работы Deep Image Prior"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dip = DeepImagePrior()\n",
                "lpips_weight, mask_size, run_button, output = create_dip_interface()\n",
                "\n",
                "def run_optimization(_):\n",
                "    with output:\n",
                "        output.clear_output()\n",
                "        \n",
                "        # Создаем маску\n",
                "        mask = torch.ones_like(ref_img)\n",
                "        mask[:, :, mask_size.value[0]:mask_size.value[1], \n",
                "             mask_size.value[0]:mask_size.value[1]] = 0.0\n",
                "        \n",
                "        # Запускаем оптимизацию\n",
                "        rec, G = dip.optimize(ref_img, lpips_weight.value, mask)\n",
                "        \n",
                "        # Показываем результаты\n",
                "        dip.visualize_progress()\n",
                "        plt.show()\n",
                "\n",
                "run_button.on_click(run_optimization)\n",
                "\n",
                "# Отображаем интерфейс\n",
                "widgets.VBox([lpips_weight, mask_size, run_button, output])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Условная генерация\n",
                "\n",
                "В этом разделе мы рассмотрим условную генерацию изображений с помощью BigGAN и изучим влияние различных параметров на качество генерации."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ConditionalGeneration:\n",
                "    \"\"\"Класс для экспериментов с условной генерацией.\"\"\"\n",
                "    \n",
                "    def __init__(self, model_path='pretrained/G_ema.pth', img_size=128):\n",
                "        self.G = make_big_gan(model_path, img_size).cuda().eval()\n",
                "        \n",
                "    def generate_with_classes(self, z, classes):\n",
                "        \"\"\"Генерация изображений с заданными классами.\n",
                "        \n",
                "        Args:\n",
                "            z (torch.Tensor): Латентные векторы\n",
                "            classes (torch.Tensor): Индексы классов\n",
                "            \n",
                "        Returns:\n",
                "            torch.Tensor: Сгенерированные изображения\n",
                "        \"\"\"\n",
                "        with torch.no_grad():\n",
                "            cl_embed = self.G.big_gan.shared(classes)\n",
                "            return self.G.big_gan(z, cl_embed)\n",
                "    \n",
                "    def interpolate_classes(self, z, class1, class2, steps=5):\n",
                "        \"\"\"Интерполяция между двумя классами.\n",
                "        \n",
                "        Args:\n",
                "            z (torch.Tensor): Латентный вектор\n",
                "            class1 (int): Первый класс\n",
                "            class2 (int): Второй класс\n",
                "            steps (int): Количество шагов интерполяции\n",
                "            \n",
                "        Returns:\n",
                "            torch.Tensor: Интерполированные изображения\n",
                "        \"\"\"\n",
                "        with torch.no_grad():\n",
                "            cl1_embed = self.G.big_gan.shared(torch.tensor([class1]).cuda())\n",
                "            cl2_embed = self.G.big_gan.shared(torch.tensor([class2]).cuda())\n",
                "            \n",
                "            alphas = torch.linspace(0, 1, steps).cuda()\n",
                "            embeddings = torch.stack([\n",
                "                torch.lerp(cl1_embed[0], cl2_embed[0], alpha)\n",
                "                for alpha in alphas\n",
                "            ])\n",
                "            \n",
                "            return self.G.big_gan(z.repeat(steps, 1), embeddings)\n",
                "            \n",
                "    def generate_with_truncation(self, num_samples, class_idx, truncation=1.0):\n",
                "        \"\"\"Генерация с усечением латентного пространства.\n",
                "        \n",
                "        Args:\n",
                "            num_samples (int): Количество сэмплов\n",
                "            class_idx (int): Индекс класса\n",
                "            truncation (float): Параметр усечения\n",
                "            \n",
                "        Returns:\n",
                "            torch.Tensor: Сгенерированные изображения\n",
                "        \"\"\"\n",
                "        with torch.no_grad():\n",
                "            tr = truncnorm(-truncation, truncation)\n",
                "            z = torch.from_numpy(\n",
                "                tr.rvs(num_samples * 512)\n",
                "            ).view([num_samples, 512]).float().cuda()\n",
                "            \n",
                "            classes = torch.full([num_samples], class_idx, dtype=torch.int64).cuda()\n",
                "            return self.generate_with_classes(z, classes)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Интерактивные эксперименты с условной генерацией"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_conditional_generation_interface():\n",
                "    class1 = widgets.IntSlider(\n",
                "        value=12,\n",
                "        min=0,\n",
                "        max=1000,\n",
                "        description='Class 1:'\n",
                "    )\n",
                "    \n",
                "    class2 = widgets.IntSlider(\n",
                "        value=200,\n",
                "        min=0,\n",
                "        max=1000,\n",
                "        description='Class 2:'\n",
                "    )\n",
                "    \n",
                "    truncation = widgets.FloatSlider(\n",
                "        value=1.0,\n",
                "        min=0.1,\n",
                "        max=2.0,\n",
                "        step=0.1,\n",
                "        description='Truncation:'\n",
                "    )\n",
                "    \n",
                "    return class1, class2, truncation\n",
                "\n",
                "generator = ConditionalGeneration()\n",
                "class1, class2, truncation = create_conditional_generation_interface()\n",
                "\n",
                "def update_generation(class1, class2, truncation):\n",
                "    # Генерируем базовые изображения\n",
                "    z = torch.randn(2, 512).cuda()\n",
                "    classes = torch.tensor([class1, class2], dtype=torch.int64).cuda()\n",
                "    imgs = generator.generate_with_classes(z, classes)\n",
                "    \n",
                "    # Генерируем интерполяцию\n",
                "    imgs_interp = generator.interpolate_classes(z[0], class1, class2)\n",
                "    \n",
                "    # Генерируем с усечением\n",
                "    imgs_truncated = generator.generate_with_truncation(8, class1, truncation)\n",
                "    \n",
                "    plt.figure(figsize=(15, 5))\n",
                "    \n",
                "    plt.subplot(131)\n",
                "    plt.imshow(to_image_grid(imgs))\n",
                "    plt.title('Original Classes')\n",
                "    plt.axis('off')\n",
                "    \n",
                "    plt.subplot(132)\n",
                "    plt.imshow(to_image_grid(imgs_interp))\n",
                "    plt.title('Class Interpolation')\n",
                "    plt.axis('off')\n",
                "    \n",
                "    plt.subplot(133)\n",
                "    plt.imshow(to_image_grid(imgs_truncated))\n",
                "    plt.title(f'Truncation={truncation:.1f}')\n",
                "    plt.axis('off')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "\n",
                "widgets.interactive(update_generation, class1=class1, class2=class2, truncation=truncation)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Манипуляции в латентном пространстве\n",
                "\n",
                "В этом разделе мы исследуем возможности манипуляции латентными векторами для управления генерацией изображений."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LatentManipulator:\n",
                "    \"\"\"Класс для манипуляций в латентном пространстве.\"\"\"\n",
                "    \n",
                "    def __init__(self, classifier_path='pretrained/regressor.pth'):\n",
                "        self.regressor = CelebaAttributeClassifier('Smiling', classifier_path).cuda().eval()\n",
                "        self.samples = []\n",
                "        \n",
                "    def collect_statistics(self, num_steps=200, batch_size=8):\n",
                "        \"\"\"Сбор статистики для обучения направления атрибута.\"\"\"\n",
                "        for latents in tqdm(torch.randn([num_steps, batch_size, 512])):\n",
                "            with torch.no_grad():\n",
                "                latents = G.style_gan2.style(latents.cuda())\n",
                "                imgs = G(latents, w_space=True)\n",
                "                probs = self.regressor.get_probs(preprocess(imgs))[:, 1]\n",
                "                \n",
                "            self.samples.extend([\n",
                "                ShiftedGSample(l, p) for l, p in zip(latents.cpu(), probs.cpu())\n",
                "            ])\n",
                "            \n",
                "    def find_attribute_direction(self, max_iter=10000):\n",
                "        \"\"\"Поиск направления атрибута с помощью SVR.\"\"\"\n",
                "        return train_normal(self.samples, max_iter)\n",
                "    \n",
                "    def apply_manipulation(self, z, direction, strength=5.0):\n",
                "        \"\"\"Применение манипуляции к латентным векторам.\"\"\"\n",
                "        with torch.no_grad():\n",
                "            w = G.style_gan2.style(z)\n",
                "            return G(w + strength * direction, w_space=True)\n",
                "    \n",
                "    def visualize_manipulation(self, z, direction, strengths=None):\n",
                "        \"\"\"Визуализация результатов манипуляции с разной силой.\n",
                "        \n",
                "        Args:\n",
                "            z (torch.Tensor): Исходные латентные векторы\n",
                "            direction (torch.Tensor): Направление манипуляции\n",
                "            strengths (list): Список значений силы манипуляции\n",
                "        \"\"\"\n",
                "        if strengths is None:\n",
                "            strengths = [-10.0, -5.0, 0.0, 5.0, 10.0]\n",
                "            \n",
                "        results = [self.apply_manipulation(z, direction, s) for s in strengths]\n",
                "        grid = torch.cat(results)\n",
                "        \n",
                "        plt.figure(figsize=(15, 3))\n",
                "        plt.imshow(to_image_grid(grid, nrow=len(strengths)))\n",
                "        plt.title('Attribute Manipulation')\n",
                "        plt.axis('off')\n",
                "        \n",
                "        # Добавляем значения силы манипуляции под изображениями\n",
                "        for i, s in enumerate(strengths):\n",
                "            plt.text(i * grid.size(-1) / len(strengths), grid.size(-2) + 10,\n",
                "                    f'λ={s:.1f}', ha='center')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Интерактивный интерфейс для манипуляций"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_manipulation_interface():\n",
                "    \"\"\"Создание интерактивного интерфейса для манипуляций.\"\"\"\n",
                "    attribute = widgets.Dropdown(\n",
                "        options=['Smiling', 'Young', 'Male', 'Glasses'],\n",
                "        value='Smiling',\n",
                "        description='Attribute:'\n",
                "    )\n",
                "    \n",
                "    strength = widgets.FloatSlider(\n",
                "        value=5.0,\n",
                "        min=-10.0,\n",
                "        max=10.0,\n",
                "        step=0.5,\n",
                "        description='Strength:'\n",
                "    )\n",
                "    \n",
                "    random_seed = widgets.IntText(\n",
                "        value=42,\n",
                "        description='Seed:'\n",
                "    )\n",
                "    \n",
                "    generate_button = widgets.Button(description='Generate')\n",
                "    output = widgets.Output()\n",
                "    \n",
                "    return attribute, strength, random_seed, generate_button, output\n",
                "\n",
                "manipulator = LatentManipulator()\n",
                "attribute, strength, random_seed, generate_button, output = create_manipulation_interface()\n",
                "\n",
                "def on_generate_click(_):\n",
                "    with output:\n",
                "        output.clear_output()\n",
                "        \n",
                "        # Устанавливаем seed для воспроизводимости\n",
                "        torch.manual_seed(random_seed.value)\n",
                "        \n",
                "        # Собираем статистику если нужно\n",
                "        if not manipulator.samples:\n",
                "            print('Collecting statistics...')\n",
                "            manipulator.collect_statistics()\n",
                "            \n",
                "        # Находим направление атрибута\n",
                "        direction = manipulator.find_attribute_direction()\n",
                "        \n",
                "        # Генерируем и визуализируем результаты\n",
                "        z = torch.randn(4, 512).cuda()\n",
                "        manipulator.visualize_manipulation(z, direction, \n",
                "                                         [-strength.value, 0, strength.value])\n",
                "        plt.show()\n",
                "\n",
                "generate_button.on_click(on_generate_click)\n",
                "\n",
                "# Отображаем интерфейс\n",
                "widgets.VBox([attribute, strength, random_seed, generate_button, output])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Анализ качества генерации\n",
                "\n",
                "В этом разделе мы проанализируем качество сгенерированных изображений с помощью различных метрик."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GenerationAnalyzer:\n",
                "    \"\"\"Класс для анализа качества генерации.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.lpips_model = lpips.LPIPS('alexnet').cuda()\n",
                "        \n",
                "    def compute_diversity(self, images, num_pairs=1000):\n",
                "        \"\"\"Вычисление метрики разнообразия на основе LPIPS.\"\"\"\n",
                "        n = len(images)\n",
                "        pairs = torch.randperm(num_pairs * 2).view(-1, 2) % n\n",
                "        distances = []\n",
                "        \n",
                "        for i, j in tqdm(pairs):\n",
                "            with torch.no_grad():\n",
                "                dist = self.lpips_model(images[i:i+1], images[j:j+1])\n",
                "                distances.append(dist.item())\n",
                "                \n",
                "        return np.mean(distances), np.std(distances)\n",
                "    \n",
                "    def compute_attribute_consistency(self, images, attribute_classifier):\n",
                "        \"\"\"Вычисление консистентности атрибутов.\"\"\"\n",
                "        with torch.no_grad():\n",
                "            probs = attribute_classifier.get_probs(preprocess(images))[:, 1]\n",
                "            return probs.mean().item(), probs.std().item()\n",
                "    \n",
                "    def analyze_batch(self, images, attribute_classifier=None):\n",
                "        \"\"\"Полный анализ батча изображений.\"\"\"\n",
                "        diversity_mean, diversity_std = self.compute_diversity(images)\n",
                "        \n",
                "        results = {\n",
                "            'diversity_mean': diversity_mean,\n",
                "            'diversity_std': diversity_std\n",
                "        }\n",
                "        \n",
                "        if attribute_classifier is not None:\n",
                "            attr_mean, attr_std = self.compute_attribute_consistency(images, \n",
                "                                                                    attribute_classifier)\n",
                "            results.update({\n",
                "                'attribute_mean': attr_mean,\n",
                "                'attribute_std': attr_std\n",
                "            })\n",
                "            \n",
                "        return results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Анализ качества генерации для разных параметров"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_generation_quality():\n",
                "    analyzer = GenerationAnalyzer()\n",
                "    results = []\n",
                "    \n",
                "    # Генерируем изображения с разными параметрами truncation\n",
                "    truncations = [0.5, 1.0, 2.0]\n",
                "    \n",
                "    for trunc in truncations:\n",
                "        print(f'\\nAnalyzing truncation={trunc}:')\n",
                "        \n",
                "        # Генерируем батч изображений\n",
                "        imgs = generator.generate_with_truncation(32, 239, trunc)\n",
                "        \n",
                "        # Анализируем качество\n",
                "        metrics = analyzer.analyze_batch(imgs, manipulator.regressor)\n",
                "        results.append((trunc, metrics))\n",
                "        \n",
                "        print(f'Diversity: {metrics[\"diversity_mean\"]:.3f} ± {metrics[\"diversity_std\"]:.3f}')\n",
                "        if 'attribute_mean' in metrics:\n",
                "            print(f'Attribute: {metrics[\"attribute_mean\"]:.3f} ± {metrics[\"attribute_std\"]:.3f}')\n",
                "            \n",
                "    return results\n",
                "\n",
                "quality_results = analyze_generation_quality()\n",
                "\n",
                "# Визуализируем результаты\n",
                "plt.figure(figsize=(10, 5))\n",
                "\n",
                "truncations = [r[0] for r in quality_results]\n",
                "diversity = [r[1]['diversity_mean'] for r in quality_results]\n",
                "attribute = [r[1].get('attribute_mean', 0) for r in quality_results]\n",
                "\n",
                "plt.plot(truncations, diversity, 'b-', label='Diversity')\n",
                "plt.plot(truncations, attribute, 'r-', label='Attribute')\n",
                "plt.xlabel('Truncation')\n",
                "plt.ylabel('Score')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.title('Generation Quality vs Truncation')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Заключение\n",
                "\n",
                "В этом ноутбуке мы:\n",
                "1. Изучили метрики качества генерации (LPIPS)\n",
                "2. Поработали с Deep Image Prior\n",
                "3. Исследовали условную генерацию\n",
                "4. Научились манипулировать латентным пространством\n",
                "5. Проанализировали качество генерации\n",
                "\n",
                "Основные выводы:\n",
                "- LPIPS лучше соответствует человеческому восприятию качества изображений\n",
                "- Deep Image Prior эффективен для задач реконструкции\n",
                "- Манипуляции в латентном пространстве позволяют управлять атрибутами\n",
                "- Существует компромисс между разнообразием и качеством генерации"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}